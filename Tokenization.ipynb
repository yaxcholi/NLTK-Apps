{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jason/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/jason/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding a Corpus in Natural Language Processing\n",
    "\n",
    "## What is a Corpus?\n",
    "\n",
    "In Natural Language Processing (NLP), a **corpus** (plural: **corpora**) refers to a large and structured set of texts that are used for linguistic research, language analysis, or machine learning purposes. The corpus can include a wide variety of written or spoken materials, such as books, articles, transcripts, and even web content. \n",
    "\n",
    "### Purpose of a Corpus\n",
    "\n",
    "- **Linguistic Analysis**: A corpus provides data that researchers can analyze to understand language patterns, usage, and structure.\n",
    "- **Model Training**: In machine learning, a corpus serves as training data for algorithms, helping them learn to recognize patterns, generate text, or make predictions.\n",
    "- **Language Resource**: It can be used to create dictionaries, thesauri, and other language resources by providing examples of word usage in context.\n",
    "\n",
    "### Example of a Corpus\n",
    "\n",
    "Consider the following text as an example of a small corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Hello my name is Jason Tagud and I am a 2nd year university student at Ulster University. I love studying A.I\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization in Natural Language Processing (NLP)\n",
    "\n",
    "Tokenization is the process of splitting text into smaller components, called tokens, which can be words, phrases, or characters. It is a crucial step in Natural Language Processing (NLP) for simplifying and structuring input data for analysis.\n",
    "\n",
    "## Types of Tokenization\n",
    "\n",
    "1. **Word Tokenization**: Splits text into individual words.\n",
    "   - Example: \"Hello, my name.\" → `[\"Hello\", \"my\", \"name\"]`\n",
    "\n",
    "2. **Sentence Tokenization**: Splits text into sentences.\n",
    "   - Example: \"Hello. I am a student.\" → `[\"Hello.\", \"I am a student.\"]`\n",
    "\n",
    "3. **Character Tokenization**: Divides text into individual characters.\n",
    "   - Example: \"Hello\" → `[\"H\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "\n",
    "## Importance\n",
    "\n",
    "- Prepares raw text for analysis.\n",
    "- Enables feature extraction for machine learning.\n",
    "- Improves performance in tasks like sentiment analysis and text classification.\n",
    "\n",
    "## Example with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello my name is Jason Tagud and I am a 2nd year university student at Ulster University.', 'I love studying A.I']\n"
     ]
    }
   ],
   "source": [
    "documents=sent_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Jason Tagud and I am a 2nd year university student at Ulster University.\n",
      "I love studying A.I\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization with the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Jason',\n",
       " 'Tagud',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " '2nd',\n",
       " 'year',\n",
       " 'university',\n",
       " 'student',\n",
       " 'at',\n",
       " 'Ulster',\n",
       " 'University',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'studying',\n",
       " 'A.I']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'Jason', 'Tagud', 'and', 'I', 'am', 'a', '2nd', 'year', 'university', 'student', 'at', 'Ulster', 'University', '.']\n",
      "['I', 'love', 'studying', 'A.I']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPunctTokenizer in NLTK\n",
    "\n",
    "`WordPunctTokenizer` is a tokenizer from the Natural Language Toolkit (NLTK) that separates words from punctuation marks, creating distinct tokens for each. This is particularly useful in Natural Language Processing (NLP) for text preprocessing.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Separates Words and Punctuation**: It effectively splits words from punctuation marks, ensuring that they are treated as separate tokens.\n",
    "- **Handles Common Punctuation**: Designed to manage various punctuation marks, such as commas, periods, exclamation marks, and question marks.\n",
    "- **Useful for Preprocessing**: Ideal for preparing text data for analysis and machine learning tasks where clear distinctions between words and punctuation are necessary.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "Here’s how to use `WordPunctTokenizer` in Python with NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'Jason', 'Tagud', 'and', 'I', 'am', 'a', '2nd', 'year', 'university', 'student', 'at', 'Ulster', 'University', '.']\n",
      "['I', 'love', 'studying', 'A', '.', 'I']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "for sentence in documents:\n",
    "    print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreebankWordTokenizer in NLTK\n",
    "\n",
    "The `TreebankWordTokenizer` is a tokenizer in NLTK that closely follows the tokenization conventions used in the Penn Treebank corpus. It is designed to handle contractions and punctuation intelligently, making it suitable for tasks where precision matters.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Handles Contractions**: Splits words like *\"don't\"* into *[\"do\", \"n't\"]*.\n",
    "- **Splits Punctuation Properly**: Separates punctuation marks such as parentheses and commas.\n",
    "- **Preserves Linguistic Conventions**: Useful when working with corpora like Penn Treebank, which require specific tokenization rules.\n",
    "\n",
    "## Example Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'Jason', 'Tagud', 'and', 'I', 'am', 'a', '2nd', 'year', 'university', 'student', 'at', 'Ulster', 'University', '.']\n",
      "['I', 'love', 'studying', 'A.I']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "for sentence in documents:\n",
    "    print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
