{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with ANN using TensorFlow and Keras\n",
    "\n",
    "In this notebook, we will create a simple Artificial Neural Network (ANN) for text classification using a small dataset. We will use TensorFlow, Keras, Pandas, and Scikit-learn to build and evaluate our model.\n",
    "\n",
    "## Install Required Libraries\n",
    "\n",
    "First, ensure you have the necessary libraries installed. You can run the following command in a Jupyter Notebook cell:\n",
    "\n",
    "```python\n",
    "!pip install tensorflow pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame for Text Classification\n",
    "\n",
    "In this section, we will create a DataFrame that contains sample text data and their corresponding labels. This DataFrame will serve as our dataset for training a simple neural network model for text classification.\n",
    "\n",
    "### Define the Data\n",
    "\n",
    "We start by defining a dictionary named `data`. This dictionary consists of two keys:\n",
    "\n",
    "1. **text**: A list of sentences that represent different opinions and statements.\n",
    "2. **label**: A list of corresponding labels that categorize the sentiment of each sentence. The possible labels include:\n",
    "   - **positive**: Indicates a positive sentiment towards programming or problem-solving.\n",
    "   - **negative**: Indicates a negative sentiment related to debugging.\n",
    "   - **neutral**: Indicates a neutral sentiment towards programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text     label\n",
      "0                   I love programming in Python  positive\n",
      "1                  Python is an amazing language  positive\n",
      "2                       I enjoy machine learning  positive\n",
      "3  Deep learning is a subset of machine learning  positive\n",
      "4                         I dislike bugs in code  negative\n",
      "5                   Debugging can be frustrating  negative\n",
      "6                        I love solving problems  positive\n",
      "7                         I prefer Java over C++   neutral\n",
      "8                    C++ is powerful but complex   neutral\n",
      "9               I enjoy reading about algorithms  positive\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        'I love programming in Python',\n",
    "        'Python is an amazing language',\n",
    "        'I enjoy machine learning',\n",
    "        'Deep learning is a subset of machine learning',\n",
    "        'I dislike bugs in code',\n",
    "        'Debugging can be frustrating',\n",
    "        'I love solving problems',\n",
    "        'I prefer Java over C++',\n",
    "        'C++ is powerful but complex',\n",
    "        'I enjoy reading about algorithms'\n",
    "    ],\n",
    "    'label': [\n",
    "        'positive', 'positive', 'positive', 'positive', 'negative',\n",
    "        'negative', 'positive', 'neutral', 'neutral', 'positive'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Cleaning Text and Converting Labels\n",
    "\n",
    "In this section, we will preprocess the text data and convert the sentiment labels into a numerical format suitable for model training. This step is crucial in preparing our data for a machine learning model.\n",
    "\n",
    "### Clean the Text Data\n",
    "\n",
    "The first step in preprocessing is to clean the text data. We define a function called `clean_text` that converts the text to lowercase, which helps standardize the data and reduces the number of unique tokens.\n",
    "\n",
    "```python\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    return text.lower()  # Convert to lowercase\n",
    "```\n",
    "Next, we apply this function to the 'text' column of our DataFrame df using the apply method. This will transform all text entries to lowercase:\n",
    "\n",
    "### Convert Labels to Numerical Format\n",
    "\n",
    "Machine learning algorithms often work better with numerical data. To facilitate this, we will convert the sentiment labels into numerical codes. We can achieve this by using pandas' categorical data type, which assigns an integer code to each category.\n",
    "\n",
    "```python\n",
    "df['label'] = df['label'].astype('category').cat.codes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text  label\n",
      "0                   i love programming in python      2\n",
      "1                  python is an amazing language      2\n",
      "2                       i enjoy machine learning      2\n",
      "3  deep learning is a subset of machine learning      2\n",
      "4                         i dislike bugs in code      0\n",
      "5                   debugging can be frustrating      0\n",
      "6                        i love solving problems      2\n",
      "7                         i prefer java over c++      1\n",
      "8                    c++ is powerful but complex      1\n",
      "9               i enjoy reading about algorithms      2\n"
     ]
    }
   ],
   "source": [
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "df['label'] = df['label'].astype('category').cat.codes\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Training and Validation Sets\n",
    "\n",
    "In this section, we will split our dataset into training and validation sets. This step is essential for evaluating the performance of our model and ensuring it generalizes well to unseen data.\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "We use the `train_test_split` function from the `sklearn.model_selection` module to divide our data. This function randomly splits the dataset into two parts: \n",
    "a training set and a validation set (or test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8, Validation samples: 2\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training samples: {len(X_train)}, Validation samples: {len(X_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Explained\n",
    "\n",
    "- **`df['text']`**: The input features (text data) that we want to use for training and validation.\n",
    "- **`df['label']`**: The target variable (labels) associated with the text data.\n",
    "- **`test_size=0.2`**: This parameter specifies the proportion of the dataset to include in the validation set. In this case, 20% of the data will be used for validation, and the remaining 80% will be used for training.\n",
    "- **`random_state=42`**: This parameter ensures that the split is reproducible. Using a fixed seed (like 42) means that you will get the same split every time you run the code, which is useful for debugging and consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "Text vectorization is a crucial step in natural language processing (NLP) that transforms text data into a numerical format that can be used by machine learning models. This process allows the model to understand and process the text data.\n",
    "\n",
    "### Setting Up Text Vectorization\n",
    "\n",
    "In this section, we will use the `TextVectorization` layer from TensorFlow's Keras library to convert our text data into a format suitable for model training.\n",
    "\n",
    "```python\n",
    "max_features = 1000  # Number of unique words\n",
    "sequence_length = 10  # Length of each input sequence\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=max_features, output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000  # Number of unique words\n",
    "sequence_length = 10  # Length of each input sequence\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=max_features, output_sequence_length=sequence_length)\n",
    "\n",
    "# Fit the vectorization layer on the training data\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "# Vectorize the text data\n",
    "X_train_vectorized = vectorize_layer(X_train)\n",
    "X_val_vectorized = vectorize_layer(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Importance of Loss Function and Optimizer in Neural Networks\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "- **Performance Measurement**: Quantifies how well the model's predictions match the actual target values, providing a numerical value that reflects model performance during training.\n",
    "  \n",
    "- **Guidance for Learning**: Calculates the difference between predicted and actual values, guiding the model to adjust its weights for improvement. A high loss indicates poor performance, while a low loss signifies better performance.\n",
    "\n",
    "- **Objective of Training**: The goal is to minimize the loss function by adjusting the model's parameters (weights and biases) based on the specific task (e.g., regression, binary classification, multi-class classification).\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "- **Parameter Update Mechanism**: Adjusts the model's parameters based on gradients computed from the loss function, dictating how weights are updated during training.\n",
    "\n",
    "- **Learning Rate Control**: Determines the size of the steps taken during weight updates. A well-chosen learning rate is critical; too large can cause instability, while too small can slow down training.\n",
    "\n",
    "- **Adaptability**: Different optimizers use varied strategies for updating parameters. For instance, optimizers like Adam dynamically adjust the learning rate based on past gradients, enhancing convergence on complex loss surfaces.\n",
    "\n",
    "## Building and Compiling the Neural Network Model\n",
    "\n",
    "In this section, we will create a neural network model for text classification using Keras. The model will be built using the Sequential API, which allows us to stack layers in a linear fashion.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "We define the model as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(max_features, 8, input_length=sequence_length),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(len(df['label'].unique()), activation='softmax')  # Output layer for multi-class classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Components Explanation\n",
    "\n",
    "- **`keras.Sequential`**: \n",
    "  - This class allows us to create a linear stack of layers for our model. \n",
    "  - Each layer will receive the output of the previous layer as input.\n",
    "\n",
    "- **`layers.Embedding(max_features, 8, input_length=sequence_length)`**:\n",
    "  - This layer converts integer-encoded words into dense vectors of fixed size.\n",
    "  - **`max_features`**: The maximum number of unique words to be considered (set earlier to 1000).\n",
    "  - **`8`**: The size of the output vectors for each word.\n",
    "  - **`input_length=sequence_length`**: Specifies the length of input sequences, which ensures that all input sequences are of the same length.\n",
    "\n",
    "- **`layers.GlobalAveragePooling1D()`**:\n",
    "  - This layer reduces the dimensionality of the output from the Embedding layer by taking the average of all time steps (words) in the input sequences.\n",
    "  - It produces a single vector for each input sequence, which is then passed to the next layer.\n",
    "\n",
    "- **`layers.Dense(8, activation='relu')`**:\n",
    "  - A fully connected layer with 8 units and a ReLU (Rectified Linear Unit) activation function.\n",
    "  - This layer introduces non-linearity to the model, allowing it to learn more complex patterns.\n",
    "\n",
    "- **`layers.Dense(len(df['label'].unique()), activation='softmax')`**:\n",
    "  - The output layer for the model, which corresponds to the number of unique classes (labels) in our dataset.\n",
    "  - **`activation='softmax'`**: This activation function converts the output into probabilities for each class, making it suitable for multi-class classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "The model is trained using the `fit` method, which adjusts the model's weights based on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7333 - loss: 1.0136 - val_accuracy: 0.5000 - val_loss: 1.0673\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7833 - loss: 0.9938 - val_accuracy: 0.5000 - val_loss: 1.0647\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7833 - loss: 0.9859 - val_accuracy: 0.5000 - val_loss: 1.0627\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4500 - loss: 1.0544 - val_accuracy: 0.5000 - val_loss: 1.0610\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7833 - loss: 0.9635 - val_accuracy: 0.5000 - val_loss: 1.0584\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4000 - loss: 1.0632 - val_accuracy: 0.5000 - val_loss: 1.0576\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5000 - loss: 1.0270 - val_accuracy: 0.5000 - val_loss: 1.0557\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7333 - loss: 0.9473 - val_accuracy: 0.5000 - val_loss: 1.0535\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7333 - loss: 0.9376 - val_accuracy: 0.5000 - val_loss: 1.0509\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6000 - loss: 0.9819 - val_accuracy: 0.5000 - val_loss: 1.0497\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_2      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m8\u001b[0m)             │         \u001b[38;5;34m8,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_2      │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │            \u001b[38;5;34m27\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,099</span> (31.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,099\u001b[0m (31.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,099</span> (31.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,099\u001b[0m (31.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X_train_vectorized, y_train, validation_data=(X_val_vectorized, y_val), epochs=10, batch_size=2)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Explained\n",
    "\n",
    "- **X_train_vectorized**: \n",
    "  - The input features (vectorized text data) used for training the model.\n",
    "\n",
    "- **y_train**: \n",
    "  - The target labels corresponding to the training data.\n",
    "\n",
    "- **validation_data**: \n",
    "  - A tuple containing the validation features and labels, used to evaluate the model's performance on unseen data during training. This helps in monitoring overfitting.\n",
    "\n",
    "- **epochs**: \n",
    "  - The number of times the model will go through the entire training dataset. In this case, the model will be trained for 10 epochs.\n",
    "\n",
    "- **batch_size**: \n",
    "  - The number of samples processed before the model's internal parameters are updated. A batch size of 2 means that the model will update its weights after processing every 2 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "Finally, we will evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.5000 - loss: 1.0700\n",
      "Validation Loss: 1.069966197013855, Validation Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val_vectorized, y_val)\n",
    "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing New Texts for Prediction\n",
    "\n",
    "To make predictions using the trained model, we need to prepare new texts similarly to how the training data was prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Text: \"I love coding in Python\" - Predicted label: neutral\n",
      "Text: \"Debugging is hard\" - Predicted label: neutral\n",
      "Text: \"Machine learning is fascinating\" - Predicted label: neutral\n"
     ]
    }
   ],
   "source": [
    "# Sample new texts for prediction\n",
    "new_texts = [\n",
    "    'I love coding in Python',\n",
    "    'Debugging is hard',\n",
    "    'Machine learning is fascinating'\n",
    "]\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'positive',\n",
    "    1: 'negative',\n",
    "    2: 'neutral'\n",
    "}\n",
    "\n",
    "# Clean and vectorize the new texts\n",
    "new_texts_cleaned = [clean_text(text) for text in new_texts]\n",
    "new_texts_vectorized = vectorize_layer(new_texts_cleaned)\n",
    "\n",
    "predictions = model.predict(new_texts_vectorized)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "predicted_strings = [label_mapping[label] for label in predicted_classes]\n",
    "\n",
    "for text, pred in zip(new_texts, predicted_strings):\n",
    "    print(f'Text: \"{text}\" - Predicted label: {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `argmax` function is a method provided by NumPy, which is a fundamental package for numerical computing in Python. It is used to return the indices of the maximum values along an axis of an array.\n",
    "\n",
    "In the context of the following code snippet:\n",
    "\n",
    "```python\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "```\n",
    "* predictions is typically a NumPy array that contains the output probabilities for each class (as produced by the softmax activation function in the final layer of your model).\n",
    "* axis=1 specifies that you want to find the index of the maximum value across the columns of the array (i.e., for each row, which class has the highest predicted probability).\n",
    "\n",
    "Thus, predictions.argmax(axis=1) will return an array of indices representing the predicted class for each input sample, based on the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
